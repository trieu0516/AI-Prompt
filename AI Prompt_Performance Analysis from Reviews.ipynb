{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9800f728",
   "metadata": {},
   "source": [
    "# This notebook is a small exercise on AI prompt using Python\n",
    "\n",
    "## PaLM 2 is used here. However, this is a trial. There are limitations on the extent of implementation scope.\n",
    "\n",
    "### The main task is to detect sentiments and to infer the key words from the text (review).\n",
    "### The data set used here is McDonald Review Sentiment from Data.world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94c66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Vertex AI Text Generation\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11f33dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function using model text-bison@001\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    temperature: float = 0,\n",
    "    project_id: str = \" \",\n",
    "    location: str = \"us-central1\",\n",
    ") -> str:\n",
    "    \"\"\"Ideation example with a Large Language Model\"\"\"\n",
    "\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    # TODO developer - override these parameters as needed:\n",
    "    parameters = {\n",
    "        \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        **parameters,\n",
    "    )\n",
    "    return(f\"{response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8303694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "\n",
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Classify the review text as either \"positive\" or \"negative\".\n",
    "Your answer must not exceed 1 single word.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df29fe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "913127e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>policies_violated:confidence</th>\n",
       "      <th>city</th>\n",
       "      <th>policies_violated_gold</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.02500</td>\n",
       "      <td>RudeService\\rOrderProblem\\rFilthy</td>\n",
       "      <td>1.0\\r0.6667\\r0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01875</td>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Terrible customer service. ξI came in at 9:30p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01806</td>\n",
       "      <td>SlowService\\rOrderProblem</td>\n",
       "      <td>1.0\\r1.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01875</td>\n",
       "      <td>na</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01875</td>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>679500008</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.00903</td>\n",
       "      <td>OrderProblem</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>Portland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I enjoyed the part where I repeatedly asked if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>679500224</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01875</td>\n",
       "      <td>SlowService\\rFilthy\\rBadFood</td>\n",
       "      <td>1.0\\r1.0\\r1.0</td>\n",
       "      <td>Houston</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worst McDonalds I've been in in a long time! D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>679500608</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01667</td>\n",
       "      <td>ScaryMcDs</td>\n",
       "      <td>0.6458</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When I am really craving for McDonald's, this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>679501257</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.02222</td>\n",
       "      <td>ScaryMcDs</td>\n",
       "      <td>0.6407</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two points right out of the gate: 1. Thuggery ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>679501402</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>42056.01875</td>\n",
       "      <td>ScaryMcDs\\rSlowService</td>\n",
       "      <td>1.0\\r0.6667</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I wanted to grab breakfast one morning before ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _golden _unit_state  _trusted_judgments  _last_judgment_at  \\\n",
       "0     679455653    False   finalized                   3        42056.02500   \n",
       "1     679455654    False   finalized                   3        42056.01875   \n",
       "2     679455655    False   finalized                   3        42056.01806   \n",
       "3     679455656    False   finalized                   3        42056.01875   \n",
       "4     679455657    False   finalized                   3        42056.01875   \n",
       "...         ...      ...         ...                 ...                ...   \n",
       "1520  679500008    False   finalized                   3        42056.00903   \n",
       "1521  679500224    False   finalized                   3        42056.01875   \n",
       "1522  679500608    False   finalized                   3        42056.01667   \n",
       "1523  679501257    False   finalized                   3        42056.02222   \n",
       "1524  679501402    False   finalized                   3        42056.01875   \n",
       "\n",
       "                      policies_violated policies_violated:confidence  \\\n",
       "0     RudeService\\rOrderProblem\\rFilthy          1.0\\r0.6667\\r0.6667   \n",
       "1                           RudeService                            1   \n",
       "2             SlowService\\rOrderProblem                     1.0\\r1.0   \n",
       "3                                    na                       0.6667   \n",
       "4                           RudeService                            1   \n",
       "...                                 ...                          ...   \n",
       "1520                       OrderProblem                       0.6754   \n",
       "1521       SlowService\\rFilthy\\rBadFood                1.0\\r1.0\\r1.0   \n",
       "1522                          ScaryMcDs                       0.6458   \n",
       "1523                          ScaryMcDs                       0.6407   \n",
       "1524             ScaryMcDs\\rSlowService                  1.0\\r0.6667   \n",
       "\n",
       "             city  policies_violated_gold  \\\n",
       "0         Atlanta                     NaN   \n",
       "1         Atlanta                     NaN   \n",
       "2         Atlanta                     NaN   \n",
       "3         Atlanta                     NaN   \n",
       "4         Atlanta                     NaN   \n",
       "...           ...                     ...   \n",
       "1520     Portland                     NaN   \n",
       "1521      Houston                     NaN   \n",
       "1522     New York                     NaN   \n",
       "1523      Chicago                     NaN   \n",
       "1524  Los Angeles                     NaN   \n",
       "\n",
       "                                                 review  \n",
       "0     I'm not a huge mcds lover, but I've been to be...  \n",
       "1     Terrible customer service. ξI came in at 9:30p...  \n",
       "2     First they \"lost\" my order, actually they gave...  \n",
       "3     I see I'm not the only one giving 1 star. Only...  \n",
       "4     Well, it's McDonald's, so you know what the fo...  \n",
       "...                                                 ...  \n",
       "1520  I enjoyed the part where I repeatedly asked if...  \n",
       "1521  Worst McDonalds I've been in in a long time! D...  \n",
       "1522  When I am really craving for McDonald's, this ...  \n",
       "1523  Two points right out of the gate: 1. Thuggery ...  \n",
       "1524  I wanted to grab breakfast one morning before ...  \n",
       "\n",
       "[1525 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:/Users/trieu/Downloads/McDonald Review.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c43c176c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I'm not a huge mcds lover, but I've been to be...\n",
       "1       Terrible customer service. ξI came in at 9:30p...\n",
       "2       First they \"lost\" my order, actually they gave...\n",
       "3       I see I'm not the only one giving 1 star. Only...\n",
       "4       Well, it's McDonald's, so you know what the fo...\n",
       "                              ...                        \n",
       "1520    I enjoyed the part where I repeatedly asked if...\n",
       "1521    Worst McDonalds I've been in in a long time! D...\n",
       "1522    When I am really craving for McDonald's, this ...\n",
       "1523    Two points right out of the gate: 1. Thuggery ...\n",
       "1524    I wanted to grab breakfast one morning before ...\n",
       "Name: review, Length: 1525, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = data[\"review\"]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3b67e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "\n",
      "negative\n",
      "negative\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1028\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[1;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:216.58.203.74:443 {grpc_message:\"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.\", grpc_status:8, created_time:\"2023-08-20T11:06:53.913008671+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(review)):\n\u001b[0;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWhat is the sentiment of the following product review, \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m        which is delimited with triple backticks?\u001b[39m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m        Give your answer as a single word, either \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m        Review text: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36mget_completion\u001b[1;34m(prompt, temperature, project_id, location)\u001b[0m\n\u001b[0;32m     11\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,  \u001b[38;5;66;03m# Temperature controls the degree of randomness in token selection.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,  \u001b[38;5;66;03m# Token limit determines the maximum amount of text output.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.8\u001b[39m,  \u001b[38;5;66;03m# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m40\u001b[39m,  \u001b[38;5;66;03m# A top_k of 1 means the selected token is the most probable among all tokens.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m TextGenerationModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-bison@001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     20\u001b[0m     prompt,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\vertexai\\language_models\\_language_models.py:267\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[1;34m(self, prompt, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    247\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m _DEFAULT_TOP_P,\n\u001b[0;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextGenerationResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;124;03m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\vertexai\\language_models\\_language_models.py:303\u001b[0m, in \u001b[0;36m_TextGenerationModel._batch_predict\u001b[1;34m(self, prompts, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[0;32m    295\u001b[0m instances \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(prompt)} \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m    296\u001b[0m prediction_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxDecodeSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopP\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopK\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[0;32m    301\u001b[0m }\n\u001b[1;32m--> 303\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39mpredictions:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\aiplatform\\models.py:1559\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[1;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   1547\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1548\u001b[0m         deployed_model_id\u001b[38;5;241m=\u001b[39mraw_predict_response\u001b[38;5;241m.\u001b[39mheaders[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1556\u001b[0m         ),\n\u001b[0;32m   1557\u001b[0m     )\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1559\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   1567\u001b[0m         predictions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   1568\u001b[0m             json_format\u001b[38;5;241m.\u001b[39mMessageToDict(item)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1573\u001b[0m         model_resource_name\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   1574\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[1;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    597\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m    598\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mendpoint),)),\n\u001b[0;32m    599\u001b[0m )\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[0;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas."
     ]
    }
   ],
   "source": [
    "# The quota limit can only process roughly 25 reviews\n",
    "\n",
    "for i in range(len(review)):\n",
    "    prompt = f\"\"\"What is the sentiment of the following product review, \n",
    "        which is delimited with triple backticks?\n",
    "\n",
    "        Give your answer as a single word, either \"positive\" or \"negative\".\n",
    "        Review text: '''{review[i]}'''\n",
    "        \"\"\"\n",
    "    print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82cfbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong order, staff, hygine\n",
      "staff, waiting time, hygine\n",
      "wrong order, waiting time, staff\n",
      "food quality, staff, drink quality\n",
      "staff, food quality, waiting time\n",
      "food quality, drink quality, staff\n",
      "waiting time, staff, wrong order\n",
      "wrong order, staff, price\n",
      "waiting time, staff, drink quality\n",
      "wrong order, food quality, staff\n",
      "staff, wrong order, food quality\n",
      "staff, wrong order, price\n",
      "staff, wrong order, waiting time\n",
      "waiting time, food quality, staff\n",
      "waiting time, food quality, staff\n",
      "waiting time, food quality, staff\n",
      "food quality, waiting time, staff\n",
      "waiting time, food quality, staff\n",
      "waiting time, drink quality, staff\n",
      "\n",
      "drink quality, food quality, staff\n",
      "\n",
      "wrong order, staff, price\n",
      "wrong order, food quality, staff\n",
      "wrong order, food quality, staff\n",
      "food quality, drink quality, hygine\n",
      "staff, waiting time, drink quality\n",
      "\n",
      "staff, waiting time, food quality\n",
      "staff, food quality, drink quality\n",
      "waiting time, food quality, staff\n",
      "staff, waiting time, wrong order\n",
      "waiting time, food quality, staff\n",
      "\n",
      "food quality, drink quality, staff\n",
      "drink quality, food quality, staff\n",
      "food quality, staff, drink quality\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1028\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[1;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.204.42:443 {created_time:\"2023-08-20T11:09:45.520164365+00:00\", grpc_status:8, grpc_message:\"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(review)):\n\u001b[0;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWhat are the top 3 issues of the following product review, \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m        which is delimited with triple backticks? The issues are categorzied as follows:\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m        waiting time, food quality, staff, drink quality, hygine, wrong order, price.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m        Review text: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36mget_completion\u001b[1;34m(prompt, temperature, project_id, location)\u001b[0m\n\u001b[0;32m     11\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,  \u001b[38;5;66;03m# Temperature controls the degree of randomness in token selection.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,  \u001b[38;5;66;03m# Token limit determines the maximum amount of text output.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.8\u001b[39m,  \u001b[38;5;66;03m# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m40\u001b[39m,  \u001b[38;5;66;03m# A top_k of 1 means the selected token is the most probable among all tokens.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m TextGenerationModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-bison@001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     20\u001b[0m     prompt,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\vertexai\\language_models\\_language_models.py:267\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[1;34m(self, prompt, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    247\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m _DEFAULT_TOP_P,\n\u001b[0;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextGenerationResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;124;03m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\vertexai\\language_models\\_language_models.py:303\u001b[0m, in \u001b[0;36m_TextGenerationModel._batch_predict\u001b[1;34m(self, prompts, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[0;32m    295\u001b[0m instances \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(prompt)} \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m    296\u001b[0m prediction_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxDecodeSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopP\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopK\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[0;32m    301\u001b[0m }\n\u001b[1;32m--> 303\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39mpredictions:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\aiplatform\\models.py:1559\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[1;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   1547\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1548\u001b[0m         deployed_model_id\u001b[38;5;241m=\u001b[39mraw_predict_response\u001b[38;5;241m.\u001b[39mheaders[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1556\u001b[0m         ),\n\u001b[0;32m   1557\u001b[0m     )\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1559\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   1567\u001b[0m         predictions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   1568\u001b[0m             json_format\u001b[38;5;241m.\u001b[39mMessageToDict(item)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1573\u001b[0m         model_resource_name\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   1574\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[1;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    597\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m    598\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mendpoint),)),\n\u001b[0;32m    599\u001b[0m )\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[0;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas."
     ]
    }
   ],
   "source": [
    "# The quota limit can only process roughly 35 reviews\n",
    "\n",
    "for i in range(len(review)):\n",
    "    prompt = f\"\"\"What are the top 3 issues of the following product review, \n",
    "        which is delimited with triple backticks? The issues are categorzied as follows:\n",
    "        waiting time, food quality, staff, drink quality, hygine, wrong order, price.\n",
    "        \n",
    "        Format your answer as a list of lower-case words separated by commas.\n",
    "        Review text: '''{review[i]}'''\n",
    "        \"\"\"\n",
    "    print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7abedb",
   "metadata": {},
   "source": [
    "## The Quota limit does not allow storing the results from the function. \n",
    "## Still, the code to store is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22143327",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detection =[]\n",
    "\n",
    "for i in range(len(review)):\n",
    "    prompt = f\"\"\"What is the sentiment of the following product review, \n",
    "        which is delimited with triple backticks?\n",
    "\n",
    "        Give your answer as a single word, either \"positive\" or \"negative\".\n",
    "        Review text: '''{review[i]}'''\n",
    "        \"\"\"\n",
    "    x = get_completion(prompt)\n",
    "    sent_detection.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6deec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = []\n",
    "\n",
    "for i in range(len(review)):\n",
    "    prompt = f\"\"\"What are the top 3 issues of the following product review, \n",
    "        which is delimited with triple backticks? The issues are categorzied as follows:\n",
    "        waiting time, food quality, staff, drink quality, hygine, wrong order, price.\n",
    "        \n",
    "        Format your answer as a list of lower-case words separated by commas.\n",
    "        Review text: '''{review[i]}'''\n",
    "        \"\"\"\n",
    "    y = get_completion(prompt)\n",
    "    analysis.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed0bcb",
   "metadata": {},
   "source": [
    "### The output can be used to provide merchants with quick and insightful information about their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d90961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sentiment                                Issue\n",
      "0   negative           wrong order, staff, hygine\n",
      "1   negative          staff, waiting time, hygine\n",
      "2   negative     wrong order, waiting time, staff\n",
      "3   negative   food quality, staff, drink quality\n",
      "4   negative    staff, food quality, waiting time\n",
      "5   negative   food quality, drink quality, staff\n",
      "6   negative     waiting time, staff, wrong order\n",
      "7   negative            wrong order, staff, price\n",
      "8   negative   waiting time, staff, drink quality\n",
      "9   positive     wrong order, food quality, staff\n",
      "10  negative     staff, wrong order, food quality\n",
      "11  negative            staff, wrong order, price\n",
      "12  negative     staff, wrong order, waiting time\n",
      "13  negative    waiting time, food quality, staff\n",
      "14  negative    waiting time, food quality, staff\n",
      "15  negative    waiting time, food quality, staff\n",
      "16  negative    food quality, waiting time, staff\n",
      "17  negative    waiting time, food quality, staff\n",
      "18  negative   waiting time, drink quality, staff\n",
      "19  negative   drink quality, food quality, staff\n",
      "20  negative            wrong order, staff, price\n",
      "21  negative     wrong order, food quality, staff\n",
      "22  negative     wrong order, food quality, staff\n",
      "23  negative  food quality, drink quality, hygine\n"
     ]
    }
   ],
   "source": [
    "sentiment = pd.read_csv(\"C:/Users/trieu/Downloads/sentiment.csv\")\n",
    "issue = pd.read_csv(\"C:/Users/trieu/Downloads/issue.csv\")\n",
    "\n",
    "board = pd.merge(sentiment, issue, left_index=True, right_index=True)\n",
    "print(board)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
